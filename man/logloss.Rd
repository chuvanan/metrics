% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/log-loss.r
\name{logloss}
\alias{logloss}
\alias{mtr_log_loss}
\alias{mtr_cross_entropy}
\alias{mtr_mean_log_loss}
\alias{mtr_mean_cross_entropy}
\title{Negative log loss, also known as logarithmic loss or cross-entropy loss.}
\usage{
mtr_log_loss(actual, predicted, eps = 1e-15)

mtr_cross_entropy(actual, predicted, eps = 1e-15)

mtr_mean_log_loss(actual, predicted, eps = 1e-15)

mtr_mean_cross_entropy(actual, predicted, eps = 1e-15)
}
\arguments{
\item{actual}{\code{[numeric]} Ground truth binary numeric vector containing
1 for the positive class and 0 for the negative class.}

\item{predicted}{\code{[numeric]} A vector of estimated probabilities.}

\item{eps}{\code{[numeric]} In case of predicted probability is equal zero
or one, log loss is undefined, so probabilities are clipped to max(eps,
min(1 - eps, p)). The default value of eps is 1e-15.}
}
\value{
A numeric vector output
}
\description{
Log loss is an effective metric for measuring the performance of a
classification model where the prediction output is a probability value
between 0 and 1.

Log loss quantifies the accuracy of a classifier by penalizing false
classifications. A perfect model would have a log loss of 0. Log loss
increases as the predicted probability diverges from the actual label. This
is the cost function used in logistic regression and neural networks.

\code{mtr_log_loss} computes the elementwise log loss between two numeric
vectors. While \code{mtr_mean_log_loss} computes the average log loss
between two numeric vectors
}
\note{
The logarithm used is the natural logarithm (base-e)
}
\examples{

## log loss for scalar inputs, see how log loss is converging to zero
mtr_log_loss(1, 0.1)
mtr_log_loss(1, 0.5)
mtr_log_loss(1, 0.9)
mtr_log_loss(1, 1)

## sample data
act <- c(0, 1, 1, 0, 0)
pred <- c(0.12, 0.45, 0.9, 0.3, 0.4)

## log loss vector
mtr_log_loss(act, pred)
mtr_cross_entropy(act, pred)

## mean log loss
mtr_mean_log_loss(act, pred)
mtr_mean_cross_entropy(act, pred)


}
\seealso{
\code{\link{mtr_brier_score}}
}
\author{
An Chu
}
