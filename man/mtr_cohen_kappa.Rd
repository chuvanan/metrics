% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kappa.r
\name{mtr_cohen_kappa}
\alias{mtr_cohen_kappa}
\title{Cohen’s Kappa}
\usage{
mtr_cohen_kappa(actual, predicted, cutoff = 0.5)
}
\arguments{
\item{actual}{\code{[numeric]} Ground truth binary numeric vector containing
1 for the positive class and 0 for the negative class.}

\item{predicted}{\code{[numeric]} A vector of estimated probabilities.}

\item{cutoff}{\code{[numeric]} A cutoff value for \code{predicted} vector
which classify a sample into a given class. Default value is 0.5}
}
\value{
A numeric scalar output
}
\description{
\code{mtr_cohen_kappa} computes Kappa statistic which measures inter-rater
agreement for categorical items.

The form of Kappa statistic is: \eqn{Kappa = (O - E) / (1 - E)} where O is
the observed accuracy and E is the expected accuracy based on the marginal
total of the confusion matrix. The statistic can take on values between -1
and 1; a value of 0 means there is no agreement between the actual and the
predicted, while a value of 1 indicates perfect concordance of the model
prediction and the observed classes.
}
\examples{

act <- c(1, 0, 1, 0, 1)
pred <- c(0.1, 0.9, 0.3, 0.5, 0.2)
mtr_cohen_kappa(act, pred)


set.seed(2093)
pred <- runif(1000)
act <- round(pred)
pred[sample(1000, 300)] <- runif(300) # noises
mtr_cohen_kappa(act, pred)

}
\references{
\itemize{
\item Max Kuhn and Kjell Johnson, Applied Predictive Modeling (New York: Springer-Verlag, 2013)
\item \href{https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english}{"Classification - Cohen’s Kappa in Plain English, Cross Validated"}
}
}
\author{
An Chu
}
