

[![Travis build status](https://travis-ci.org/chuvanan/metrics.svg?branch=master)](https://travis-ci.org/chuvanan/metrics)
[![Codecov test coverage](https://codecov.io/gh/chuvanan/metrics/branch/master/graph/badge.svg)](https://codecov.io/gh/chuvanan/metrics?branch=master)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)

## metrics

### Introduction

**Why another package for evaluating machine learning models?**

Because I believe there's still a niche for an R package that have all the
following traits in one place:

- *Simple.*

- *Consistent interface.*

- *Well-documented.*

- *Well-tested.*

- *Accurate and fast*

Why do I think so? It is arguably that there's no single R package is on a par
with scikit-learn's `metrics` module.



**How is your package different?**



### How to use `metrics`

#### Installation

Install the stable version of `metrics` from CRAN:

```{r, eval=FALSE}
install.packages("metrics")
```

Or install the development version from Github with:

```{r, eval=FALSE}
devtools::install_github("chuvanan/metrics")
```

#### Getting started

All `metrics` functions share the same interface: `mtr_fun(actual, predicted,
cutoff = 0.5)` which is applicable to both classification and regression
settings.

* `mtr_` is the short form of **m**e**tr**ics. As in `stringr` package,
  `metrics` uses the prefix to provide consistent naming that makes it easy to
  type with autocompletion in RStuido or Emacs's ESS.

* `_fun` is the name of performance metrics. The package declares verbosely
  which measure is going to be used. For a full list of evaluation metrics,
  please see TODO.

* `metrics` package prefers convention over configuration. Argument `actual`, in
  context of classification tasks, stricly accepts binary values `0` and `1`
  where the former is negative class and the later is positive one.

Here's a quick example:

```{r}
library(metrics)

## simulate sample data set
set.seed(123)
preds <- runif(1000)
truth <- round(preds)
preds[sample(1000, 300)] <- runif(300) # noise

## overall accuracy
mtr_accuracy(truth, preds)

## precision & recall
mtr_precision(truth, preds)
mtr_recall(truth, preds)

## AUROC
mtr_auc_roc(truth, preds)

```
