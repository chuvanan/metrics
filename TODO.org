


* List of performance metrics

** Classification problems

Metrics that built around confusion matrix:

- [X] Confusion Matrix (True Positive, True Negative, False Positive - Type I Error, False Negative - Type II Error)

- [X] True Positive Rate (TPR) / Hit Rate / Recall / Sensitivity / Detection Rate

- [X] True Negative Rate (TNR) / Specificity / Selectivity

- [X] False Positive Rate (FPR) / Fall-out / False Alarm Rate (FAR)

- [X] False Negative Rate (FNR) / Miss Rate

- [X] Accuracy / Correct Classification Rate (Correctly Classified Rate)

- [X] Misclassification Rate

- [X] Balanced Accuracy

- [X] Positive Predicted Value (PPV) / Precision

- [X] Average Precision

- [X] False Discovery Rate (FDR)

- [X] False Omission Rate (FOR)

- [X] Prevalence

- [X] F1 Score

- [X] Matthews Correlation Coefficient (MCC)

- [X] Informedness (Bookmaker Informedness - BM) / Youden Index (Youden's J Statistic)

- [X] Markedness (MK)

- [X] AUROC (Area Under ROC)

- [X] AR (Accuracy Ratio) / Gini Coefficient

- [X] KS Statistic

- [ ] AUPR (Area Under Precision-Recall Curve)

- [ ] Bayesian Error Rate

- [X] Fowlkes-Mallows Index / G-Score

Proper scoring rule:

- [X] Log Loss & Mean Log Loss

- [X] Brier Score

** Regression problems

* References

- [[https://martin-thoma.com/binary-classifier-evaluation/][Evaluation of binary classifiers]]
