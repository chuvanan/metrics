{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../tinytest/two-class-sample-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = data[\"act\"]\n",
    "probability = data[\"pred\"]\n",
    "prediction = np.where(probability > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    1\n",
      "Name: act, dtype: int64\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(truth[1:6])\n",
    "print(prediction[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.838\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "acc_score = metrics.accuracy_score(truth, prediction)\n",
    "print(\"Accuracy Score:\", acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.8395408938887199\n"
     ]
    }
   ],
   "source": [
    "## auc\n",
    "auc = metrics.roc_auc_score(truth, probability)\n",
    "print(\"AUROC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision Score: 0.7813020508395832\n"
     ]
    }
   ],
   "source": [
    "## average precision score\n",
    "avg_precision_score = metrics.average_precision_score(truth, prediction)\n",
    "print(\"Average Precision Score:\", avg_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score: 0.837904658270791\n"
     ]
    }
   ],
   "source": [
    "## balanced accuracy score\n",
    "balanced_acc_score = metrics.balanced_accuracy_score(truth, prediction)\n",
    "print(\"Balanced Accuracy Score:\", balanced_acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6758093165415819"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.balanced_accuracy_score(truth, prediction, adjusted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier Score Loss: 0.16388947565578774\n"
     ]
    }
   ],
   "source": [
    "## brier score loss\n",
    "brier_score = metrics.brier_score_loss(truth, probability)\n",
    "print(\"Brier Score Loss:\", brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa Score: 0.6759066611184021\n"
     ]
    }
   ],
   "source": [
    "## cohen kappa score\n",
    "cohen_kappa_score = metrics.cohen_kappa_score(truth, prediction)\n",
    "print(\"Cohen Kappa Score:\", cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[428,  78],\n",
       "       [ 84, 410]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.835030549898167\n"
     ]
    }
   ],
   "source": [
    "## F1 score\n",
    "f1_score = metrics.f1_score(truth, prediction)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 0.5285336531342744\n"
     ]
    }
   ],
   "source": [
    "## Log loss\n",
    "log_loss = metrics.log_loss(truth, probability)\n",
    "print(\"Log Loss:\", log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews correlation coefficient 0.6759553597038589\n"
     ]
    }
   ],
   "source": [
    "## Matthews correlation coefficient (MCC)\n",
    "matthews_corr = metrics.matthews_corrcoef(truth, prediction)\n",
    "print(\"Matthews correlation coefficient\", matthews_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8401639344262295\n"
     ]
    }
   ],
   "source": [
    "## Precision\n",
    "precision = metrics.precision_score(truth, prediction)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8401639344262295"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(truth, prediction, average = 'binary', pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8299595141700404\n"
     ]
    }
   ],
   "source": [
    "## Recall\n",
    "recall = metrics.recall_score(truth, prediction, pos_label  = 1)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-One Loss: 0.16200000000000003\n"
     ]
    }
   ],
   "source": [
    "## Zero-one loss\n",
    "zero_one_loss = metrics.zero_one_loss(truth, prediction)\n",
    "print(\"Zero-One Loss:\", zero_one_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of thresholds vector: 283\n",
      "[0.         0.         0.00197628 0.00197628 0.00395257]\n",
      "[0.00202429 0.01012146 0.01012146 0.01619433 0.01619433]\n",
      "[0.99774516 0.99590932 0.99375648 0.99160932 0.99115638]\n",
      "1.9977451590821151\n"
     ]
    }
   ],
   "source": [
    "## compute roc curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(truth, probability, pos_label=1)\n",
    "print(\"Length of thresholds vector:\", len(thresholds))\n",
    "print(fpr[1:6])\n",
    "print(tpr[1:6])\n",
    "print(thresholds[1:6])\n",
    "print(max(thresholds)) # = max(probability) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge Loss: 0.668\n"
     ]
    }
   ],
   "source": [
    "## Hinge loss\n",
    "hinge_loss = metrics.hinge_loss(truth, prediction)\n",
    "print(\"Hinge Loss:\", hinge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss: 0.162\n"
     ]
    }
   ],
   "source": [
    "## Hamming loss\n",
    "hamming_loss = metrics.hamming_loss(truth, prediction)\n",
    "print(\"Hamming Loss:\", hamming_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score: 0.838\n"
     ]
    }
   ],
   "source": [
    "## Jaccard score\n",
    "jaccard_score = metrics.jaccard_similarity_score(truth, prediction)\n",
    "print(\"Jaccard Score:\", jaccard_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84       506\n",
      "           1       0.84      0.83      0.84       494\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1000\n",
      "   macro avg       0.84      0.84      0.84      1000\n",
      "weighted avg       0.84      0.84      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Classification metrics\n",
    "print(metrics.classification_report(truth, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.494     , 0.84016393, 1.        ]),\n",
       " array([1.        , 0.82995951, 0.        ]),\n",
       " array([0, 1]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## precision-recall curve\n",
    "metrics.precision_recall_curve(truth, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine all metrics into one data frame\n",
    "sklearn_metrics = pd.DataFrame(data = {\n",
    "    'accuracy_score':acc_score,\n",
    "    'auc':auc,\n",
    "    'average_precision_score':avg_precision_score,\n",
    "    'balanced_accuracy':balanced_acc_score,\n",
    "    'brier_score':brier_score,\n",
    "    'cohen_kappa_score':cohen_kappa_score,\n",
    "    'f1_score':f1_score,\n",
    "    'hamming_loss':hamming_loss,\n",
    "    'hinge_loss':hinge_loss,\n",
    "    'jaccard_score':jaccard_score,\n",
    "    'log_loss':log_loss,\n",
    "    'matthews_corrcoef':matthews_corr,\n",
    "    'precision':precision,\n",
    "    'recall':recall,\n",
    "    'zero_one_loss':zero_one_loss\n",
    "}, index = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_metrics.to_csv(\"../tinytest/sklearn-metrics-two-class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve = pd.DataFrame(data = {\n",
    "    'fpr':fpr,\n",
    "    'tpr':tpr,\n",
    "    'thresholds':thresholds\n",
    "})\n",
    "roc_curve.to_csv(\"../tinytest/sklearn-metrics-roc-curve.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (47, 2)\n",
      "Column names: Index(['act', 'pred'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "reg_sample_data = pd.read_csv(\"../tinytest/regression-sample-data.csv\")\n",
    "print(\"Shape of data:\", reg_sample_data.shape)\n",
    "print(\"Column names:\", reg_sample_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation: 0.706735001592725\n"
     ]
    }
   ],
   "source": [
    "## explained variation\n",
    "explained_variation = metrics.explained_variance_score(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"Explained variation:\", explained_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Error: 92.5\n"
     ]
    }
   ],
   "source": [
    "## max error\n",
    "max_error = np.max(np.abs(np.array(reg_sample_data['act']), np.array(reg_sample_data['pred'])))\n",
    "print(\"Max Error:\", max_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.2, 83.1, 92.5, 85.8, 76.9, 76.1, 83.8, 92.4, 82.4, 82.9, 87.1,\n",
       "       64.1, 66.9, 68.9, 61.7, 68.3, 71.7, 55.7, 54.3, 65.1, 65.5, 65. ,\n",
       "       56.6, 57.4, 72.5, 74.2, 72. , 60.5, 58.3, 65.4, 75.5, 69.3, 77.3,\n",
       "       70.5, 79.4, 65. , 92.2, 79.3, 70.4, 65.7, 72.7, 64.4, 77.6, 67.6,\n",
       "       35. , 44.7, 42.8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(reg_sample_data['act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error 5.32138023658641\n"
     ]
    }
   ],
   "source": [
    "## mean absolute error\n",
    "mean_abs_error = metrics.mean_absolute_error(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"Mean Absolute Error\", mean_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 44.78814745625719\n"
     ]
    }
   ],
   "source": [
    "## mean squared error\n",
    "mean_squared_error = metrics.mean_squared_error(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"Mean Squared Error:\", mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Log Error: 0.009589264084385303\n"
     ]
    }
   ],
   "source": [
    "## mean sqaured log error\n",
    "mean_squared_log_error = metrics.mean_squared_log_error(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"Mean Squared Log Error:\", mean_squared_log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Absolute Error: 4.990128583411703\n"
     ]
    }
   ],
   "source": [
    "## median absolute error\n",
    "median_abs_error = metrics.median_absolute_error(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"Median Absolute Error:\", median_abs_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.706735001592725\n"
     ]
    }
   ],
   "source": [
    "## R2\n",
    "r2 = metrics.r2_score(reg_sample_data['act'], reg_sample_data['pred'])\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine all regression metrics into data frame\n",
    "reg_metrics_df = pd.DataFrame({\n",
    "    'explained_variation':explained_variation,\n",
    "    'max_error':max_error,\n",
    "    'mean_absolute_error':mean_abs_error,\n",
    "    'mean_squared_error':mean_squared_error,\n",
    "    'mean_squared_log_error':mean_squared_log_error,\n",
    "    'median_absolute_error':median_abs_error,\n",
    "    'r2':r2\n",
    "}, index = [1])\n",
    "reg_metrics_df.to_csv(\"../tinytest/sklearn-metrics-regression.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
